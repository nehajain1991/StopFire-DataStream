{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing using Apache Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by : Mayank Bhardwaj, Neha Jain, Rishupal Singh Chabbra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the implementation of streaming application in Apache Spark Streaming which has a local streaming context with two execution threads and a batch interval of 10 seconds. The streaming application will receive streaming data from all three producers designed Producer 1, Producer 2 and Producer 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the data from three producers, \n",
    "* if we have data from atleast P1, we join it with the incoming streams of P2 and P3 based on the locations. \n",
    "* Additionally, geohash function is applied on the longitude and latitude values. \n",
    "* If the data from P2 and P3 is present, the average of both the values is taken. \n",
    "* Data for both P1 and (P2+P3) stored in the database using the data model that was created as a part Task A(referencing model). \n",
    "* If P2 and P3 both do not exists, P1 stream data is sent to databases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic \"climate\" is used for the Kafka stream similar to producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pprint import pprint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash as gh\n",
    "\n",
    "# joining two json records based on longitude and latitude\n",
    "def joinjson(one, two ):\n",
    "    result = []\n",
    "    match = False\n",
    "\n",
    "    if two['latitude'] == one['latitude'] and two['longitude'] == one['longitude']:\n",
    "            match = True\n",
    "               \n",
    "    return match\n",
    "\n",
    "\n",
    "# loading the hotspot data to MongoDB after processing\n",
    "def sendDataToDB_hotspot(data):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    hotspot = db.hotspot\n",
    "        \n",
    "    y = hotspot.count()\n",
    "\n",
    "    index = str(y+1)\n",
    "    data = json.dumps(data)\n",
    "    data = json.loads(data)\n",
    "    jsonData = {}\n",
    "    jsonData[\"_id\"] = index\n",
    "    jsonData['latitude']= data['latitude']\n",
    "    jsonData['longitude']= data['longitude']\n",
    "    jsonData['datetime']=data['created_time']\n",
    "    jsonData['confidence']= data['confidence']\n",
    "    jsonData['surface_temperature_celcius']= data['surface_temperature_celcius']\n",
    "    jsonData['climate'] = data['climate']\n",
    "\n",
    "    try:\n",
    "        hotspot.replace_one({\"_id\":index}, jsonData, True)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "\n",
    "# loading the climate data to MongoDB after processing\n",
    "def sendDataToDB_climate(data):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    climate = db.climate\n",
    "    \n",
    "    y = climate.count()\n",
    "\n",
    "    index = str(y+1)\n",
    "    data = json.dumps(data)\n",
    "    data = json.loads(data)\n",
    "    jsonData = {}\n",
    "    jsonData[\"_id\"] = index\n",
    "    jsonData['latitude']= data['latitude']\n",
    "    jsonData['longitude']= data['longitude']\n",
    "    jsonData['air_temperature_celcius']=data['air_temperature_celcius']\n",
    "    jsonData['relative_humidity']= data['relative_humidity']\n",
    "    jsonData['windspeed_knots']= data['windspeed_knots']\n",
    "    jsonData['max_wind_speed']=data['max_wind_speed']\n",
    "    jsonData['precipitation']=data['precipitation']\n",
    "    jsonData['sender_id']= data['sender_id']\n",
    "    jsonData['datetime'] = data['created_time']\n",
    "\n",
    "    try:\n",
    "        climate.replace_one({\"_id\":index}, jsonData, True)\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    client.close()\n",
    "    return index\n",
    "\n",
    "# processing the stream of the data\n",
    "def processStream(iter):\n",
    "\n",
    "    joinresultP2 = []\n",
    "    P1 = []       \n",
    "    P2 = []\n",
    "    P3 = []\n",
    "    for message in iter: # iteratig over all the records in the streams\n",
    "        message = message[1]\n",
    "        if 'P1' in message:\n",
    "            P1.append(message)\n",
    "        if 'P2' in message:\n",
    "            P2.append(message)\n",
    "        if 'P3' in message:\n",
    "            P3.append(message)\n",
    "\n",
    "    if (len(P1) >= 1):\n",
    "        # if P1 exists, send to database\n",
    "        for each in P1:\n",
    "            each = json.loads(each)\n",
    "            lat = float(each[\"latitude\"])\n",
    "            lon = float(each[\"longitude\"])\n",
    "            each[\"geohash\"] = gh.encode(lat, lon, precision = 5)\n",
    "            index = sendDataToDB_climate(each)\n",
    "            \n",
    "            if (len(P2) >= 1):\n",
    "                # if P2 exists, join with P1\n",
    "                for each1 in P2:\n",
    "                    each1 = json.loads(each1)\n",
    "                    match = joinjson(each, each1)\n",
    "                    if match:\n",
    "                        each1['climate'] = index\n",
    "                        joinresultP2.append(each1)\n",
    "                           \n",
    "            if(len(P3) >= 1):\n",
    "                # if P3 exists, join with P1\n",
    "                for each1 in P3:\n",
    "                    each1 = json.loads(each1)\n",
    "                    match = joinjson(each, each1)\n",
    "                    if match:\n",
    "                        each1['climate'] = index\n",
    "                        joinresultP2.append(each1)\n",
    "                                              \n",
    "        # if both P2 and P3 exists, take average\n",
    "        if(len(joinresultP2) > 1):\n",
    "            l = len(joinresultP2)\n",
    "            surface = 0\n",
    "            conf = 0\n",
    "            for i in range(l):\n",
    "                record = joinresultP2[i]\n",
    "                surface = surface + float(record[\"surface_temperature_celcius\"])\n",
    "                conf = conf + float(record['confidence'])\n",
    "            \n",
    "            surface = surface/l\n",
    "            conf = conf/l\n",
    "            \n",
    "            record = joinresultP2[0]\n",
    "            record[\"surface_temperature_celcius\"] = str(surface)\n",
    "            record['confidence'] = str(conf)\n",
    "        \n",
    "            joinresultP2 = [record]\n",
    "            \n",
    "        for each in joinresultP2:\n",
    "            sendDataToDB_hotspot(each)\n",
    "\n",
    "            \n",
    "#batch interval of 10 seconds\n",
    "n_secs = 10\n",
    "topic = 'climate' # topic\n",
    "# Spark ocntext with 2 threads\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\").set(\"spark.streaming.concurrentJobs\", \"3\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "# Kafka object to fetch data in streams\n",
    "kafkaStream1 = KafkaUtils.createDirectStream(ssc, [topic],{\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "\n",
    "# processing for each batch\n",
    "lines1 = kafkaStream1.foreachRDD(lambda rdd: rdd.foreachPartition(processStream))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
